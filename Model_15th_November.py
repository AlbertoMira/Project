# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oxxx4hTcD1qqRxS1xCf3584EkeiWGDZZ
"""

# Import libraries.
import tensorflow as tf
from tensorflow import keras
import IPython
!pip install -q -U keras-tuner
import kerastuner as kt

# Load the dataset and split it into train and test.
# It is important to note that we have a dataset of 28x28 images, since we will have to take account of this afterwards, especially at the time
# of passing the data onto our model.
fashion_mnist = keras.datasets.fashion_mnist 
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

# Due to the fact that the images contain pixel values between 0 and 255, it would be great if we make the pixel values go from 0 to 1,
# so it is easier to compute.
train_images = train_images / 255.0
test_images = test_images / 255.0

# Then, we can build our model. In this case, it is a sequential one, consisting of 1 input layer, 1 hidden layer and 1 output layer.
# As we can see, it contains several parameters that are already set, like the numbers of neurons of each layer, the optimizer, the loss function, etc.
# Finally, it is well noting that since we intend to classify 10 things, the last layer should contain 10 nodes, as stated below, meaning that
# for our project, since we intend to classify 5 movements, that layer should contain 5 nodes instead of 10.
model = keras.Sequential([
  keras.layers.Flatten(input_shape=(28, 28)),
  keras.layers.Dense(256, activation='relu'), 
  keras.layers.Dense(10, activation='softmax') 
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Now, once our model is defined, we can start training it by using the data we have already processed.
model.fit(train_images, train_labels, epochs=10)

# After training, it comes the test process, meaning that once our model learned, it is time for it to recognize unseen data.
test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=1) 
print('Test accuracy:', test_acc)

# As we can see, the testing accuracy is lower than the training one. This is because we previously overfitted our model.
# For that purpose, we can choose the parameters which best fit our model (hyptertuning).
# For this purpose, we will use the Keras Tuner.

def model_builder(hp):
  model = keras.Sequential()
  model.add(keras.layers.Flatten(input_shape=(28, 28)))

  # Choose the best number of units on the first dense layer.
  hp_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)
  model.add(keras.layers.Dense(units = hp_units, activation = 'relu'))
  model.add(keras.layers.Dense(10))

  # Choose a proper learning rate. In this case, we will choose between 0.01, 0.001, or 0.0001.
  hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) 

  model.compile(optimizer = keras.optimizers.Adam(learning_rate = hp_learning_rate),
                loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), 
                metrics = ['accuracy'])

  return model

tuner = kt.Hyperband(model_builder,
                     objective = 'val_accuracy', 
                     max_epochs = 10,
                     factor = 3,
                     directory = 'my_dir',
                     project_name = 'intro_to_kt')

# We also need to define a callback in order to clear the training outputs in every training step.
class ClearTrainingOutput(tf.keras.callbacks.Callback):
  def on_train_end(*args, **kwargs):
    IPython.display.clear_output(wait = True)

# Get the optimal hyperparameters
tuner.search(train_images, train_labels, epochs = 10, validation_data = (test_images, test_labels), callbacks = [ClearTrainingOutput()])

best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

# Build the model with the best parameters and train the model.
model = tuner.hypermodel.build(best_hps)
model.fit(train_images, train_labels, epochs = 10, validation_data = (test_images, test_labels))

# Test the model
test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=1) 
print('Test accuracy:', test_acc)

# Finally, we see how there is no difference between the training accuracy and the test one, so we can say that we got rid of the initial overfitting.